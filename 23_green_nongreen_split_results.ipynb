{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, precision_score\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Green Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(\"data/english_annotated_full_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/model_splits/green_split/green_as_train/green_test_predictions.csv\")\n",
    "df_inference = pd.read_csv(\"data/model_splits/green_split/green_as_train/green_inference_predictions.csv\")\n",
    "df_train = pd.read_csv(\"data/model_splits/green_split/green_as_train/train-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predictions (green): How well does the model perform? Are the predictions significantly different than the actual codes regarding environmental protection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.6430, 0.6784, 0.6810, 0.6644, 0.6455, 0.6777,  0.6700, 0.6644, 0.6552, 0.6667,\n",
    "           0.6473, 0.6265, 0.6655, 0.6259, 0.6395, 0.6661, 0.6706, 0.6562, 0.6626, 0.6632]\n",
    "val_loss = [0.4035, 0.4631, 0.5046, 0.7762, 0.8961, 0.8864, 0.8025, 1.1034, 1.1414, 1.2555,\n",
    "            1.2923, 1.3150, 1.1883, 1.3822, 1.4309, 1.3541, 1.3666, 1.4396, 1.4340, 1.4371]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (binary)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor Green Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy:\", accuracy_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set precision:\", precision_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set recall:\", recall_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set F1-score:\", f1_score(df_test[\"label\"], df_test[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of target code in the training data:\", df_train[df_train[\"green_code\"] == 1].shape[0]/df_train.shape[0])\n",
    "print(\"Percentage of target code in real codes:\", df_test[df_test[\"label\"] == 1].shape[0]/df_test.shape[0])\n",
    "print(\"Percentage of target code in predicted codes:\", df_test[df_test[\"preds\"] == 1].shape[0]/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"[[True Neg -- False Pos]\\n[ False Neg -- True Pos]]\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: What real codes do they have?\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] == 0)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "codes_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false positives in Green Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "target_code = 411\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_test[df_test[\"preds\"] == 0].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 1].shape[0]],\n",
    "                                  \"Coders\": [df_test[df_test[\"label\"] == 0].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 1].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows: the model creates predictions that are very similarly distributed compared to the original codes, as we would have expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Predictions (green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy:\", accuracy_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Test set precision:\", precision_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Test set recall:\", recall_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Test set F1-score:\", f1_score(df_inference[\"label\"], df_inference[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of target code in the training data:\", df_train[df_train[\"green_code\"] == 1].shape[0]/df_train.shape[0])\n",
    "print(\"Percentage of target code in real codes:\", df_inference[df_inference[\"label\"] == 1].shape[0]/df_inference.shape[0])\n",
    "print(\"Percentage of target code in predicted codes:\", df_inference[df_inference[\"preds\"] == 1].shape[0]/df_inference.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"[[True Neg -- False Pos]\\n[ False Neg -- True Pos]]\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This now has a lot of false positives (as we would expect)\n",
    "# Better look at the False Positives: What real codes do they have?\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] == 0)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "codes_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false positives in Green Model inference set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "target_code = 703\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference[df_inference[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 1].shape[0]],\n",
    "                                  \"Coders\": [df_inference[df_inference[\"label\"] == 0].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 1].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predictions are very clearly significantly different that the real predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph showing change in false positive predictions\n",
    "all_codes = set(corpus_df[\"main_codes\"].unique())\n",
    "\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] == 0)]\n",
    "d_test = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "# add missing codes:\n",
    "d_test = pd.concat([d_test, pd.Series(0, index=all_codes-set(d_test.index))]).sort_index()\n",
    "\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] == 0)]\n",
    "d_inf = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "# add missing codes:\n",
    "d_inf = pd.concat([d_inf, pd.Series(0, index=all_codes-set(d_inf.index))]).sort_index()\n",
    "\n",
    "# difference going from test (green) to inf (non-green)\n",
    "d_diff = (d_inf - d_test).sort_values()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [416, 703]\n",
    "d_test_selection = d_test.loc[interesting_codes]\n",
    "d_inf_selection = d_inf.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (Green manifestos)\": d_test_selection*100, \"Inference set (non-Green manifestos)\": d_inf_selection*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['darkgreen', 'grey'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Green model: Frequency of select codes in the false positives')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['416\\nAnti Growth Economy: Positive', '703\\nAgriculture and Farmers']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the same thing, but for the NonGreen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.7055, 0.7137, 0.7095, 0.7247, 0.6759, 0.7188, 0.7112, 0.7202, 0.7147, 0.7049,\n",
    "           0.7059, 0.7013, 0.6979, 0.7301, 0.7189, 0.7112, 0.7099, 0.7217, 0.7269, 0.7215]\n",
    "val_loss = [0.0767, 0.0995, 0.1216, 0.1275, 0.1521, 0.1643, 0.1660, 0.1855, 0.1845, 0.2165,\n",
    "            0.2271, 0.2345, 0.2636, 0.2493, 0.2563, 0.2856, 0.2809, 0.3033, 0.3207, 0.3214]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (binary)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor non-Green Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/model_splits/green_split/non_green_as_train/nongreen_test_predictions.csv\")\n",
    "df_inference = pd.read_csv(\"data/model_splits/green_split/non_green_as_train/nongreen_inference_predictions.csv\")\n",
    "df_train = pd.read_csv(\"data/model_splits/green_split/non_green_as_train/train-00000-of-00001.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions (NonGreen): How well does the model perform? Are the predictions significantly different than the actual codes regarding environmental protection?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy:\", accuracy_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set precision:\", precision_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set recall:\", recall_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set F1-score:\", f1_score(df_test[\"label\"], df_test[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of target code in the training data:\", df_train[df_train[\"green_code\"] == 1].shape[0]/df_train.shape[0])\n",
    "print(\"Percentage of target code in real codes:\", df_test[df_test[\"label\"] == 1].shape[0]/df_test.shape[0])\n",
    "print(\"Percentage of target code in predicted codes:\", df_test[df_test[\"preds\"] == 1].shape[0]/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"[[True Neg -- False Pos]\\n[ False Neg -- True Pos]]\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: What real codes do they have?\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] == 0)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "codes_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false positives in Non-Green Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "target_code = 411\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_test[df_test[\"preds\"] == 0].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 1].shape[0]],\n",
    "                                  \"Coders\": [df_test[df_test[\"label\"] == 0].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 1].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model overpredicts 501 already here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here the model does differ significantly from the predictions, but not an insane amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference predictions (Non Green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inference set accuracy:\", accuracy_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Inference set precision:\", precision_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Inference set recall:\", recall_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Inference set F1-score:\", f1_score(df_inference[\"label\"], df_inference[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of target code in the training data:\", df_train[df_train[\"green_code\"] == 1].shape[0]/df_train.shape[0])\n",
    "print(\"Percentage of target code in real codes:\", df_inference[df_inference[\"label\"] == 1].shape[0]/df_inference.shape[0])\n",
    "print(\"Percentage of target code in predicted codes:\", df_inference[df_inference[\"preds\"] == 1].shape[0]/df_inference.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting that our model overpredicts in this case as well. We would definitely expect it to underpredict (if coders give less 501 codes to Non-Green parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"[[True Neg -- False Pos]\\n[ False Neg -- True Pos]]\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the False Negatives (need to look at the actual texts to find patterns) might be good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: What real codes do they have?\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] == 0)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "codes_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false positives in Green Model inference set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "416 (anti-growth/sustainability): likely coded as 501 more often when party is Green!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "target_code = 416\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference[df_inference[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 1].shape[0]],\n",
    "                                  \"Coders\": [df_inference[df_inference[\"label\"] == 0].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 1].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting MORE environmental codes compared to the coders. This is not what we would expect tbh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less significantly different than when comparing to the test set, also not what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph showing change in false positive predictions\n",
    "all_codes = set(corpus_df[\"main_codes\"].unique())\n",
    "\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] == 0)]\n",
    "d_test = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "# add missing codes:\n",
    "d_test = pd.concat([d_test, pd.Series(0, index=all_codes-set(d_test.index))]).sort_index()\n",
    "\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] == 0)]\n",
    "d_inf = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "# add missing codes:\n",
    "d_inf = pd.concat([d_inf, pd.Series(0, index=all_codes-set(d_inf.index))]).sort_index()\n",
    "\n",
    "# difference going from test (non-green) to inf (green)\n",
    "d_diff = (d_inf - d_test).sort_values()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [416, 703]\n",
    "d_test_selection = d_test.loc[interesting_codes]\n",
    "d_inf_selection = d_inf.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (non-Green manifestos)\": d_test_selection*100, \"Inference set (Green manifestos)\": d_inf_selection*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['grey', 'darkgreen'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Non-Green model: Frequency of select codes in the false positives')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['416\\nAnti Growth Economy: Positive', '703\\nAgriculture and Farmers']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left party model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.7050, 0.7093, 0.7035, 0.7061, 0.7004, 0.6936, 0.7046, 0.6979, 0.7019, 0.7046,\n",
    "           0.7030, 0.7085, 0.6997, 0.7046, 0.7032, 0.7036, 0.7038, 0.7072, 0.7051, 0.7066]\n",
    "val_loss = [0.6205, 0.7307, 0.8032, 1.0851, 1.3747, 1.7668, 1.7892, 2.0678, 2.2576, 2.2779,\n",
    "            2.4922, 2.6629, 2.7028, 2.7458, 2.7886, 2.9100, 2.8942, 2.9562, 3.0273, 3.0530]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (macro)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor Left Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right party model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.7832, 0.8012, 0.8108, 0.8148, 0.8045, 0.8170, 0.8123, 0.8187, 0.8165, 0.8121,\n",
    "           0.8155, 0.8216, 0.8128, 0.8247, 0.8271, 0.8251, 0.8234, 0.8218, 0.8218, 0.8243]\n",
    "val_loss = [0.5295, 0.5219, 0.6031, 0.7936, 1.0773, 1.1831, 1.3600, 1.4785, 1.6175, 1.6854,\n",
    "            1.6336, 1.6960, 1.8910, 1.8448, 1.8517, 1.9199, 1.9848, 2.0593, 2.0637, 2.0698]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (macro)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor Right Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
