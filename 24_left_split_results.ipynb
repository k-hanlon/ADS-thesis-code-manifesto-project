{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, precision_score\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Left Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RILE:\n",
    "neutral = 0\n",
    "left = 1\n",
    "right = 2\n",
    "\n",
    "Parties:\n",
    "    center = 0\n",
    "    left = 1\n",
    "    right = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/model_splits/left_right_split/left_as_train/left_test_predictions.csv\")\n",
    "df_inference = pd.read_csv(\"data/model_splits/left_right_split/left_as_train/left_inference_right_predictions.csv\")\n",
    "df_inference_center = pd.read_csv(\"data/model_splits/left_right_split/left_as_train/left_inference_center_predictions.csv\")\n",
    "df_train = pd.read_csv(\"data/model_splits/left_right_split/left_as_train/train-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference_center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predictions: How well does the model perform? Are the predictions significantly different than the actual codes regarding RILE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.7050, 0.7093, 0.7035, 0.7061, 0.7004, 0.6936, 0.7046, 0.6979, 0.7019, 0.7046,\n",
    "           0.7030, 0.7085, 0.6997, 0.7046, 0.7032, 0.7036, 0.7038, 0.7072, 0.7051, 0.7066]\n",
    "val_loss = [0.6205, 0.7307, 0.8032, 1.0851, 1.3747, 1.7668, 1.7892, 2.0678, 2.2576, 2.2779,\n",
    "            2.4922, 2.6629, 2.7028, 2.7458, 2.7886, 2.9100, 2.8942, 2.9562, 3.0273, 3.0530]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (macro)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor Left Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy:\", accuracy_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set precision:\", precision_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))\n",
    "print(\"Test set recall:\", recall_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))\n",
    "print(\"Test set F1-score:\", f1_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in test predictions:\")\n",
    "df_test[\"preds\"].value_counts()/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in real test labels:\")\n",
    "df_test[\"label\"].value_counts()/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, higher chance of predicting Neutral by mistake than going from left to right/right to left (makes sense!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Left Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 305 is a RIGHT code (Political Authority: Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 305\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 2) & (df_test[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Left Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 504 (Welfare State Expansion), 202 (Democracy), 403 (Market Regulation) are LEFT categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 202\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_test[df_test[\"preds\"] == 0].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 1].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_test[df_test[\"label\"] == 0].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 1].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predictions on test set are significantly different to real labels (more left/right, less neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Predictions (Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inf set accuracy:\", accuracy_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Inf set precision:\", precision_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set recall:\", recall_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set F1-score:\", f1_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Right) predictions:\")\n",
    "df_inference[\"preds\"].value_counts()/df_inference.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Right) real labels:\")\n",
    "df_inference[\"label\"].value_counts()/df_inference.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurence of right stable, but more left and less neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Left Model Inference-Right set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 505\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 2) & (df_inference[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Left Model Inference-Right set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for 403, rather big changes here... (maybe make graph showing risers/fallers in false positive distributions between test and inference sets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 504\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference[df_inference[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 1].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_inference[df_inference[\"label\"] == 0].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 1].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very close in number of right predictions! But large differences between left and neutral (Maybe count the concrete number going from which to what and compare these between test/inf sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predictions are very clearly significantly different that the real predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Predictions (Center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inf set accuracy:\", accuracy_score(df_inference_center[\"label\"], df_inference_center[\"preds\"]))\n",
    "print(\"Inf set precision:\", precision_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set recall:\", recall_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set F1-score:\", f1_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Center) predictions:\")\n",
    "df_inference_center[\"preds\"].value_counts()/df_inference_center.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Center) real labels:\")\n",
    "df_inference_center[\"label\"].value_counts()/df_inference_center.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More left predictions, less neutral predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference_center[\"label\"], df_inference_center[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_inference_center[(df_inference_center[\"preds\"] == 1) & (df_inference_center[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Left Model Inference-Center set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 505\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_inference_center[(df_inference_center[\"preds\"] == 2) & (df_inference_center[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Left Model Inference-Center set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 504\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference_center[df_inference_center[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference_center[df_inference_center[\"preds\"] == 1].shape[0],\n",
    "                                            df_inference_center[df_inference_center[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_inference_center[df_inference_center[\"label\"] == 0].shape[0],\n",
    "                                             df_inference_center[df_inference_center[\"label\"] == 1].shape[0],\n",
    "                                             df_inference_center[df_inference_center[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite close in number of right predictions! But large differences between left and neutral (Maybe count the concrete number going from which to what and compare these between test/inf sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unterschiede in den False-Positive Distributions von Test zu Center zu Right (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False left predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = set(df_train[\"main_codes\"].unique())\n",
    "\n",
    "# test:\n",
    "tmp = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] != 1)]\n",
    "false_left_test = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "# add missing codes:\n",
    "false_left_test = pd.concat([false_left_test, pd.Series(0, index=all_codes-set(false_left_test.index))]).sort_index()\n",
    "\n",
    "# inference center\n",
    "tmp = df_inference_center[(df_inference_center[\"preds\"] == 1) & (df_inference_center[\"label\"] != 1)]\n",
    "false_left_center = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_center = pd.concat([false_left_center, pd.Series(0, index=all_codes-set(false_left_center.index))]).sort_index()\n",
    "\n",
    "# inference right\n",
    "tmp = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] != 1)]\n",
    "false_left_right = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_right = pd.concat([false_left_right, pd.Series(0, index=all_codes-set(false_left_right.index))]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the main codes and calculate the difference (so going from test to center and test to right)\n",
    "test_to_center = (false_left_center - false_left_test).sort_values(ascending=False)\n",
    "test_to_right = (false_left_right - false_left_test).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [505, 401, 501, 706]\n",
    "d_test_selection = false_left_test.loc[interesting_codes]\n",
    "d_inf_center_selection = false_left_center.loc[interesting_codes]\n",
    "d_inf_right_selection = false_left_right.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (Left manifestos)\": d_test_selection*100,\n",
    "                       \"Inference set (Center manifestos)\": d_inf_center_selection*100,\n",
    "                       \"Inference set (Right manifestos)\": d_inf_right_selection*100})\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['lightgrey', 'grey', 'black'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Left model: Frequency of select codes in the false left predictions')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['505\\nWelfare State Limitation', '401\\nFree Market Economy:\\nPositive',\n",
    "              '501\\nEnvironmental Protection:\\nPositive', '706\\nNon-Economic\\nDemographic Groups:\\nPositive']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Right predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = set(df_train[\"main_codes\"].unique())\n",
    "\n",
    "# test:\n",
    "tmp = df_test[(df_test[\"preds\"] == 2) & (df_test[\"label\"] != 2)]\n",
    "false_left_test = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "# add missing codes:\n",
    "false_left_test = pd.concat([false_left_test, pd.Series(0, index=all_codes-set(false_left_test.index))]).sort_index()\n",
    "\n",
    "# inference center\n",
    "tmp = df_inference_center[(df_inference_center[\"preds\"] == 2) & (df_inference_center[\"label\"] != 2)]\n",
    "false_left_center = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_center = pd.concat([false_left_center, pd.Series(0, index=all_codes-set(false_left_center.index))]).sort_index()\n",
    "\n",
    "# inference right\n",
    "tmp = df_inference[(df_inference[\"preds\"] == 2) & (df_inference[\"label\"] != 2)]\n",
    "false_left_right = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_right = pd.concat([false_left_right, pd.Series(0, index=all_codes-set(false_left_right.index))]).sort_index()\n",
    "\n",
    "# sort by the main codes and calculate the difference (so going from test to center and test to right)\n",
    "test_to_center = (false_left_center - false_left_test).sort_values(ascending=False)\n",
    "test_to_right = (false_left_right - false_left_test).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [410, 703, 504, 705]\n",
    "d_test_selection = false_left_test.loc[interesting_codes]\n",
    "d_inf_center_selection = false_left_center.loc[interesting_codes]\n",
    "d_inf_right_selection = false_left_right.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (Left manifestos)\": d_test_selection*100,\n",
    "                       \"Inference set (Center manifestos)\": d_inf_center_selection*100,\n",
    "                       \"Inference set (Right manifestos)\": d_inf_right_selection*100})\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['lightgrey', 'grey', 'black'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Left model: Frequency of select codes in the false right predictions')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['410\\nEconomic Growth', '703\\nAgriculture and Farmers',\n",
    "              '504\\nWelfare State Expansion', '705\\nMinority Groups:\\nPositive']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
