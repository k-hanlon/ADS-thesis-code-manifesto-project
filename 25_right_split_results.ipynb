{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, precision_score\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Left Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RILE:\n",
    "neutral = 0\n",
    "left = 1\n",
    "right = 2\n",
    "\n",
    "Parties:\n",
    "    center = 0\n",
    "    left = 1\n",
    "    right = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/model_splits/left_right_split/right_as_train/right_test_predictions.csv\")\n",
    "df_inference = pd.read_csv(\"data/model_splits/left_right_split/right_as_train/right_inference_left_predictions.csv\")\n",
    "df_inference_center = pd.read_csv(\"data/model_splits/left_right_split/right_as_train/right_inference_center_predictions.csv\")\n",
    "df_train = pd.read_csv(\"data/model_splits/left_right_split/right_as_train/train-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference_center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predictions: How well does the model perform? Are the predictions significantly different than the actual codes regarding RILE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training graph:\n",
    "val_f1s = [0.7832, 0.8012, 0.8108, 0.8148, 0.8045, 0.8170, 0.8123, 0.8187, 0.8165, 0.8121,\n",
    "           0.8155, 0.8216, 0.8128, 0.8247, 0.8271, 0.8251, 0.8234, 0.8218, 0.8218, 0.8243]\n",
    "val_loss = [0.5295, 0.5219, 0.6031, 0.7936, 1.0773, 1.1831, 1.3600, 1.4785, 1.6175, 1.6854,\n",
    "            1.6336, 1.6960, 1.8910, 1.8448, 1.8517, 1.9199, 1.9848, 2.0593, 2.0637, 2.0698]\n",
    "epochs = range(1,21)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting the first dataset with left y-axis\n",
    "ax1.plot(epochs, val_f1s, 'g-')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('F1 Score (macro)', color='g')\n",
    "\n",
    "# Creating a second y-axis with shared x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Setting x-axis ticks every two steps\n",
    "ax1.set_xticks(range(0, len(epochs)+1, 2))\n",
    "ax2.set_xticks(range(0, len(epochs)+1, 2))\n",
    "\n",
    "plt.title('Validation F1-Score and Validation Loss\\nfor Right Party Model training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy:\", accuracy_score(df_test[\"label\"], df_test[\"preds\"]))\n",
    "print(\"Test set precision:\", precision_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))\n",
    "print(\"Test set recall:\", recall_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))\n",
    "print(\"Test set F1-score:\", f1_score(df_test[\"label\"], df_test[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in test predictions:\")\n",
    "df_test[\"preds\"].value_counts()/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in real test labels:\")\n",
    "df_test[\"label\"].value_counts()/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, higher chance of predicting Neutral by mistake than going from left to right/right to left (makes sense!)\n",
    "\n",
    "But real neutral mistakes are evenly distributed between left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Right Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 601 is a RIGHT code (National Way of Life: Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 601\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_test[(df_test[\"preds\"] == 2) & (df_test[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Right Model test set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 504 (Welfare State Expansion), 403 (Market Regulation) are LEFT categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 403\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the model does does not seem too great tbh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_test[df_test[\"preds\"] == 0].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 1].shape[0],\n",
    "                                            df_test[df_test[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_test[df_test[\"label\"] == 0].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 1].shape[0],\n",
    "                                             df_test[df_test[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Predictions (Left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inf set accuracy:\", accuracy_score(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "print(\"Inf set precision:\", precision_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set recall:\", recall_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set F1-score:\", f1_score(df_inference[\"label\"], df_inference[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (left) predictions:\")\n",
    "df_inference[\"preds\"].value_counts()/df_inference.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (left) real labels:\")\n",
    "df_inference[\"label\"].value_counts()/df_inference.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference[\"label\"], df_inference[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Right Model Inference-Left set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "503: Equality Positive is very often seen as a left code (maybe more a problem of RILE... as this code is seen as neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 503\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_inference[(df_inference[\"preds\"] == 2) & (df_inference[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Right Model Inference-Left set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 504\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference[df_inference[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 1].shape[0],\n",
    "                                            df_inference[df_inference[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_inference[df_inference[\"label\"] == 0].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 1].shape[0],\n",
    "                                             df_inference[df_inference[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predictions are very clearly significantly different that the real predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Predictions (Center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inf set accuracy:\", accuracy_score(df_inference_center[\"label\"], df_inference_center[\"preds\"]))\n",
    "print(\"Inf set precision:\", precision_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set recall:\", recall_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))\n",
    "print(\"Inf set F1-score:\", f1_score(df_inference_center[\"label\"], df_inference_center[\"preds\"], average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in training data:\")\n",
    "df_train[\"RILE\"].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Center) predictions:\")\n",
    "df_inference_center[\"preds\"].value_counts()/df_inference_center.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RILE distribution in inference (Center) real labels:\")\n",
    "df_inference_center[\"label\"].value_counts()/df_inference_center.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Pred. Neg:   Pred. Pos\n",
    "#    Real Neg:       True Neg --- False Pos\n",
    "#    Real Pos:       False Neg --- True Pos\n",
    "print(\"                Pred. Neutral   Pred. Left  Pred. Right\")\n",
    "print(\"Real Neutral:\")\n",
    "print(\"Real Left:\")\n",
    "print(\"Real Right:\")\n",
    "print(\"\\nAbsolut confusion matrix\\n\", confusion_matrix(df_inference_center[\"label\"], df_inference_center[\"preds\"]))\n",
    "#print(\"Relativ confusion matrix\\n\", confusion_matrix(df_test[\"label\"], df_test[\"preds\"])/df_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED LEFT but REAL RIGHT/CENTER\n",
    "df_false_pos = df_inference_center[(df_inference_center[\"preds\"] == 1) & (df_inference_center[\"label\"] != 1)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false left predictions in Right Model Inference-Center set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 503\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better look at the False Positives: PREDICTED RIGHT but REAL LEFT/CENTER\n",
    "df_false_pos = df_inference_center[(df_inference_center[\"preds\"] == 2) & (df_inference_center[\"label\"] != 2)]\n",
    "codes_distributions = df_false_pos[\"main_codes\"].value_counts()/df_false_pos.shape[0]\n",
    "print(codes_distributions[0:10])\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "codes_distributions[0:5].plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Codes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of real codes for false right predictions in Right Model Inference-Center set')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at examples\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "target_code = 504\n",
    "df_false_pos[df_false_pos[\"main_codes\"] == target_code].tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the predictions significantly differ from the real codes\n",
    "\n",
    "# set up contingency table\n",
    "contingency_table = pd.DataFrame({#\"Group\": [\"# 501 codes\", \"# non-501 codes\"],\n",
    "                                  \"Model\": [df_inference_center[df_inference_center[\"preds\"] == 0].shape[0],\n",
    "                                            df_inference_center[df_inference_center[\"preds\"] == 1].shape[0],\n",
    "                                            df_inference_center[df_inference_center[\"preds\"] == 2].shape[0]],\n",
    "                                  \"Coders\": [df_inference_center[df_inference_center[\"label\"] == 0].shape[0],\n",
    "                                             df_inference_center[df_inference_center[\"label\"] == 1].shape[0],\n",
    "                                             df_inference_center[df_inference_center[\"label\"] == 2].shape[0]]})\n",
    "\n",
    "\n",
    "contingency_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unterschiede in den False-Positive Distributions von Test zu Center zu Left (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False left predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = set(df_train[\"main_codes\"].unique())\n",
    "\n",
    "# test:\n",
    "tmp = df_test[(df_test[\"preds\"] == 1) & (df_test[\"label\"] != 1)]\n",
    "false_left_test = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "# add missing codes:\n",
    "false_left_test = pd.concat([false_left_test, pd.Series(0, index=all_codes-set(false_left_test.index))]).sort_index()\n",
    "\n",
    "# inference center\n",
    "tmp = df_inference_center[(df_inference_center[\"preds\"] == 1) & (df_inference_center[\"label\"] != 1)]\n",
    "false_left_center = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_center = pd.concat([false_left_center, pd.Series(0, index=all_codes-set(false_left_center.index))]).sort_index()\n",
    "\n",
    "# inference left\n",
    "tmp = df_inference[(df_inference[\"preds\"] == 1) & (df_inference[\"label\"] != 1)]\n",
    "false_left_right = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_right = pd.concat([false_left_right, pd.Series(0, index=all_codes-set(false_left_right.index))]).sort_index()\n",
    "\n",
    "# sort by the main codes and calculate the difference (so going from test to center and test to right)\n",
    "test_to_center = (false_left_center - false_left_test).sort_values(ascending=False)\n",
    "test_to_right = (false_left_right - false_left_test).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [503, 301, 601, 401]\n",
    "d_test_selection = false_left_test.loc[interesting_codes]\n",
    "d_inf_center_selection = false_left_center.loc[interesting_codes]\n",
    "d_inf_right_selection = false_left_right.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (Right manifestos)\": d_test_selection*100,\n",
    "                       \"Inference set (Center manifestos)\": d_inf_center_selection*100,\n",
    "                       \"Inference set (Left manifestos)\": d_inf_right_selection*100})\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['lightgrey', 'grey', 'black'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Right model: Frequency of select codes in the false left predictions')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['503\\nEquality: Positive', '301\\nDecentralisation:\\nPositive',\n",
    "              '601\\nNational Way of Life:\\nPositive', '401\\nFree-Market Economy\\nPositive']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False right predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = set(df_train[\"main_codes\"].unique())\n",
    "\n",
    "# test:\n",
    "tmp = df_test[(df_test[\"preds\"] == 2) & (df_test[\"label\"] != 2)]\n",
    "false_left_test = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "# add missing codes:\n",
    "false_left_test = pd.concat([false_left_test, pd.Series(0, index=all_codes-set(false_left_test.index))]).sort_index()\n",
    "\n",
    "# inference center\n",
    "tmp = df_inference_center[(df_inference_center[\"preds\"] == 2) & (df_inference_center[\"label\"] != 2)]\n",
    "false_left_center = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_center = pd.concat([false_left_center, pd.Series(0, index=all_codes-set(false_left_center.index))]).sort_index()\n",
    "\n",
    "# inference left\n",
    "tmp = df_inference[(df_inference[\"preds\"] == 2) & (df_inference[\"label\"] != 2)]\n",
    "false_left_right = tmp[\"main_codes\"].value_counts()/tmp.shape[0]\n",
    "false_left_right = pd.concat([false_left_right, pd.Series(0, index=all_codes-set(false_left_right.index))]).sort_index()\n",
    "\n",
    "# sort by the main codes and calculate the difference (so going from test to center and test to right)\n",
    "test_to_center = (false_left_center - false_left_test).sort_values(ascending=False)\n",
    "test_to_right = (false_left_right - false_left_test).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_codes = [504, 411, -1, 410]\n",
    "d_test_selection = false_left_test.loc[interesting_codes]\n",
    "d_inf_center_selection = false_left_center.loc[interesting_codes]\n",
    "d_inf_right_selection = false_left_right.loc[interesting_codes]\n",
    "df_tmp = pd.DataFrame({\"Test set (Right manifestos)\": d_test_selection*100,\n",
    "                       \"Inference set (Center manifestos)\": d_inf_center_selection*100,\n",
    "                       \"Inference set (Left manifestos)\": d_inf_right_selection*100})\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = df_tmp.plot(kind='bar', color=['lightgrey', 'grey', 'black'], figsize=(10, 6))\n",
    "\n",
    "# Customizing labels and title\n",
    "ax.set_xlabel('Code')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Right model: Frequency of select codes in the false right predictions')\n",
    "\n",
    "# add % to y axis ticks\n",
    "ticks = ax.get_yticks()\n",
    "percent_ticks = [f'{int(t)}%' for t in ticks]\n",
    "ax.set_yticklabels(percent_ticks)\n",
    "\n",
    "new_labels = ['504\\nWelfare State Expansion', '411\\nTechnology & Infrastructure:\\nPositive',\n",
    "              '-1\\nNo meaningful category', '410\\nEconomic Growth']\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
